SCHEDULE:
  OPTIMIZER:
    type: "AdamW"
    lr: 0.0001
    betas: [0.9, 0.999]
    weight_decay: 0.05
    PARAMWISE_CFG:
      custom_keys: [{}]
  OPTIMIZER_CONFIG:
    grad_clip: ""
    fp16: True
  LR_POLICY:
    policy: "CosineAnnealing"
    by_epoch: False
    min_lr_ratio: 0.001
    warmup: "linear"
    warmup_iters: 2000
    warmup_ratio: 0.001
    warmup_by_epoch: False
  RUNNER:
    type: "EpochBasedRunner"
    max_epochs: 24
